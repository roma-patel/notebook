
any pair of speakers and listeners in actuality are likely to be influenced by distinct background knowledge. a goal-oriented communication framework assumes that they start from scratch, learn a _language_ and reach equilibrium (i.e., manage to coordinate) by sharing beliefs and the other's knowledge of the world. we can break this down in two ways; first, can a final speaker and final listener sampled from different worlds coordinate with each other? and second, can a speaker and listener who start from scratch, but have seperate knowledge aspects coordinate with each other? note that the second can be a softer RSA model with added parameters.

---

what we know: 

with limited knowledge, coordination might not always take place. consider a world with two referents, two utterances and skewed priors and costs. there is an innate bias towards listeners picking highest prior referent, speakers picking the lowest cost utterance; but with more parameters or more knowledge, this might change.  


---

kanwisher talk: there exist specialisations in the brain that respond to specific mental functions, although this cannot be exhaustive (there exist mental functions, even those that may be critical to survival, that don't - or have not, so far - been proven to respond). there exists a general portion of the brain that specialises in processing language, but what she showed recently are developments of a finer granularity. a smaller part of the portion fires when we _understand_ the meaning of a sentence, another smaller part fires when you do mental arithmetic, memorise concepts, understand music and so on. what is hugely relevant: there exists a part that fires when you think about what another person is thinking. 

---

the above neural portrait of the human mind supports everything we do with RSA, POMDPs and any framework that allows speakers and listeners to coordinate as a conseqence of viewing and updating belief states.  
