Want to handle noisy annotations from different workers for the same text. Two complementary tasks for noisy annotations: 1. aggregate sequential crowd labels to infer a best single set of annotations i.e. consider a set of noisy annotations and predict a single gold standard annotated statement 2. use crowd annotations as training data for a model to predict sequences in unannotated text i.e. train on the annotations and create a model to predict annotated sequences. For aggregation use a novel HMM variant, for prediction use a neural approach based on LSTM

1 Introduction
Crowdsourced annotations can be noisy, therefore want to model crowdsourced label quality to estimate reliability of an individual annotator and to aggregate individual annotation to induce a single set of “reference standard” consensus labels i.e. from all individual annotations, try to create a single gold standard.
Approach is to augment sequence labelling models like HMMS and LSTMS by introducing a “crowd component”. For HMMS, model this component by including additional parameters for label quality of a worker and crowd label variables (??). For LSTMs, introduce a vector representation for each annotator. Therefore in both cases the crowd component models both the noise from the labels and the quality of a label of each annotator. 
For evaluation look at NER in news and IE from medical abstracts. 

3 Methods
First define a standard HMM with hidden states hi, observations vi and transition parameter vectors tau and emission parameter vectors omega. Therefore hi+1|hi is a discrete distribution over tau and vi|hi is a discrete distribution over omega. The discrete distributions are governed by multinomials. vi is the word at position i and hi is the true, latent class of vi. For the crowd component assume that there are n classes and let lij be the label for word i provided by worker j. C(j) is the confusion matrix for worker j. And C_k(j) is a vector of size n in which element k’ is the probability of worker j providing the label k’ for a word whose true class is k. Therefore lij|hi is a discrete distribution with parameter C_hi(j)

Factor graph of the model (HMM crowd): Assume that individual crowd worker labels are conditionally independent given the hidden true label. To keep parameter estimation efficient collapse the confusion matrix into a confusion vector. For a worker j, instead of a n cross n matrix C(j) use a n cross 1 vector C’(j), where C’_k(j) is the P of worker j labelling a word with true class k correctly. Smooth the estimate of C’ with prior counts. 
Learning: Use the EM algorithm to learn parameters (tau, omega and C’) given the observations. In the E-step, given the current estimates of the parameters, take a forward and backward pass in the HMM to infer the hidden stats. Therefore Let alpha(h_i) = p(h_i| v_1:i, l_1:i) where v_1:i are words from position 1 to i and l are the crowd labels for these words from all workers. Let beta(h_i) = p(v_i+1:n, l_i+1:n|h_i). 
The recursion are: alpha(h_i) = sum_(h_i-1) p(vi|hi)p(hi|hi-1) product_j p(lij|hi)alpha(hi-1). Therefore sum over previous states, and product over workers who have provided labels for word i or i+1. The posterior probabilities are then easily evaluated: p(h_i|V,L) ~ alpha(hi)beta(hi) and p(hi,hi+1|V, L) ~ alpha(hi)p(hi+1|hi)p(vi+1|hi+1)beta(hi+1)
beta(h_i) = sum(h_i+1) p(h_i+1|hi)p(vi+1|hi+1) product_j(p(l_i+1,j|h_i+1)beta(h_i+1)


