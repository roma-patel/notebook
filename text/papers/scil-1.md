recent years have seen a surge in NLI tasks (...). these entailment tasks are informal but intuitive notions of inference, different from strict logical operations and expressions. There is more contextual focus and need for local inference in these tasks, with an emphasis on the semantic meaning of arguments and variability of linguistic expression. However, models trained to perform such inferences, seem to understand the _semantics_ of arguments in question, but not the core underlying logical relations. For example, most NLI models seem to understand that _some_ and _all_ are semantically similar in that they can both be used to quantify amounts of objects. However, the same models fail to correctly identify relations between sentences that contain them i.e., _all_ entails _some_, _some_ may not entail _all_, _some_ does not contradict _all_ and so on. 

The ability to draw simple inferences is a key test of natural language understanding. For entailment tasks, if models cannot recognise that the premise entails the hypothesis, they cannot have really understood the premise (or the hypothesis). Moreover, merely understanding the semantics of constituents (e.g., _some_ is semantically more similar to _all_ than to _grass_, or some other word that exists in the pair) is not enough, if the logical relation encoded in that context is not realised. 

Here we focus on comparison and quantification logical relations and determine the ability of different models when tested on NLI sentence pairs that specifically test the above phenomena. We consider models that are objectively different from each other in terms of the form of input and signal they receive (text, image features, syntactic structures etc.), the objective they are trained for (entailment, language modeling, image captioning, ccg parsing) and therefore the output they are trained to produce (e.g., predetermined classes, words, syntacic structures etc.). In order to test 
